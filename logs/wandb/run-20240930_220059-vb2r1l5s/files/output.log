Casting to class labels: 100%|████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 421495.73 examples/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████| 49.0/49.0 [00:00<00:00, 430kB/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████████| 465/465 [00:00<00:00, 2.18MB/s]
vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████| 213k/213k [00:00<00:00, 714kB/s]
tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████| 436k/436k [00:00<00:00, 1.42MB/s]
/Users/justn/Library/Caches/pypoetry/virtualenvs/genai-detect-distilroberta-ZN1zM0B8-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Map: 100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:08<00:00, 978.06 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1129.60 examples/s]
/Users/justn/Library/Caches/pypoetry/virtualenvs/genai-detect-distilroberta-ZN1zM0B8-py3.11/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/justn/genai-detect-distilroberta/checkpoints exists and is not empty.

  | Name          | Type                               | Params | Mode
-----------------------------------------------------------------------------
0 | model         | PeftModelForSequenceClassification | 66.5 M | train
1 | metrics       | MetricCollection                   | 0      | train
2 | train_metrics | MetricCollection                   | 0      | train
3 | valid_metrics | MetricCollection                   | 0      | train
4 | test_metrics  | MetricCollection                   | 0      | train
-----------------------------------------------------------------------------
702 K     Trainable params
65.8 M    Non-trainable params
66.5 M    Total params
265.943   Total estimated model params size (MB)
206       Modules in train mode
98        Modules in eval mode
Epoch 4: 100%|█| 667/667 [08:31<00:00,  1.31it/s, v_num=1l5s, train_BinaryAccuracy=0.875, train_BinaryF1Score=0.909, train_BinaryPr
/Users/justn/Library/Caches/pypoetry/virtualenvs/genai-detect-distilroberta-ZN1zM0B8-py3.11/lib/python3.11/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
                                                                                                                                   
`Trainer.fit` stopped: `max_epochs=5` reached.
